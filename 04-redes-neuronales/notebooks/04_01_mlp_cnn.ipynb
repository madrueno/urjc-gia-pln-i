{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema 4: Redes neuronales para clasificación de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yjamnpb4yi7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PATH_MODELS = Path.cwd().parent / 'models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96fb38b",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "Entrenar un modelo simple de red neuronal (MLP) utilizando Keras de TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34a6d8",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Carga de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Estoy un poco harto del día a día , nada mejora',\n",
    "             'Hoy es un buen día',\n",
    "             'No se te ve satisfecho con el trabajo',\n",
    "             'Este paisaje es hermoso y bonito']\n",
    "\n",
    "# 1: positivo, 0: negativo\n",
    "labels = [0, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da0f4f9",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Funciones de normalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6054132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_filtered(token):\n",
    "    return not (token.is_punct | token.is_space | token.is_stop | len(token.text) < 4)\n",
    "\n",
    "def spacy_processing(doc, filtering, lematization):\n",
    "    tokens = []\n",
    "    if filtering and lematization:\n",
    "        tokens = [token.lemma_ for token in doc if token_filtered(token)]\n",
    "    elif lematization:\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "    elif filtering:\n",
    "        tokens = [token.text for token in doc if token_filtered(token)]\n",
    "    else:\n",
    "        tokens = [token.text for token in doc]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9f1f3",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Preparar datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def prepare_vocabulary(corpus, vocab_size, normalize):\n",
    "    token_to_index = {}\n",
    "    current_index = 1\n",
    "\n",
    "    for sentence in corpus:\n",
    "        doc = nlp(sentence)\n",
    "        if normalize:\n",
    "            doc = nlp(spacy_processing(doc, True, True))\n",
    "        for token in doc:\n",
    "            if token.text not in token_to_index and current_index < vocab_size:\n",
    "                token_to_index[token.text] = current_index\n",
    "                current_index += 1\n",
    "\n",
    "    return token_to_index\n",
    "\n",
    "\n",
    "def prepare_sentences(corpus, vocabulary, max_length, normalize):\n",
    "    encoded_sentences = []\n",
    "\n",
    "    for sentence in corpus:\n",
    "        doc = nlp(sentence)\n",
    "        if normalize:\n",
    "            doc = nlp(spacy_processing(doc, True, True))\n",
    "        encoded_sentence = []\n",
    "        for token in doc:\n",
    "            if token.text in vocabulary:\n",
    "                encoded_sentence.append(vocabulary[token.text])\n",
    "            else:\n",
    "                encoded_sentence.append(0)\n",
    "        encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "    prepared_sentences = pad_sequences(encoded_sentences, maxlen=max_length, padding='post', truncating='post')\n",
    "    print(\"Oraciones originales(\", len(corpus), \"):\")\n",
    "    print(corpus)\n",
    "    print(\"Oraciones procesadas(\", len(prepared_sentences), \"):\")\n",
    "    print(prepared_sentences)\n",
    "    return prepared_sentences\n",
    "\n",
    "\n",
    "vocab_size = 50\n",
    "max_length = 10\n",
    "\n",
    "vocabulary_train = prepare_vocabulary(sentences, vocab_size, True)\n",
    "print(\"\\nVocabulario (\", len(vocabulary_train), \"):\")\n",
    "print(vocabulary_train)\n",
    "prepared_sentences = prepare_sentences(sentences, vocabulary_train, max_length, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1984662a",
   "metadata": {},
   "source": [
    "### Apartado d\n",
    "Configurar el modelo MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7d3532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9774d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "vector_size = 8\n",
    "model.add(Embedding(vocab_size, vector_size))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(\"Red diseñada correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4694b4",
   "metadata": {},
   "source": [
    "### Apartado e\n",
    "Compilar y entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64851cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.build(input_shape=(None, max_length))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501adcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "prepared_sentences = np.array(prepared_sentences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(prepared_sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6c6db",
   "metadata": {},
   "source": [
    "### Apartado f\n",
    "Evaluar el modelo con nuevas frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3585b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"No fui al estreno de la película porque nadie me quería acompañar\",\n",
    "    \"Envidio de buena manera a los que tienen la oportunidad de ir mañana al estadio\",\n",
    "    \"Se nos está volviendo costumbre del domingo por la noche, ver el episodio anterior de SNL y eso me hace recibir el lunes con mejor humor\",\n",
    "    \"Al final decidí no ir al cine porque estaba cansada\",\n",
    "    \"Todo es maravilloso y formidable, muy bonito\"\n",
    "]\n",
    "\n",
    "print(\"\\nVocabulario (\", len(vocabulary_train), \"):\")\n",
    "print(vocabulary_train)\n",
    "\n",
    "prepared_test = prepare_sentences(test_sentences, vocabulary_train, max_length, True)\n",
    "\n",
    "predictions = model.predict(prepared_test)\n",
    "\n",
    "print(\"Predicciones detalladas:\")\n",
    "for i, sentence in enumerate(test_sentences):\n",
    "    pred = predictions[i][0]\n",
    "    sentiment = \"Positivo\" if pred > 0.5 else \"Negativo\"\n",
    "    print(f\"\\nTexto: {sentence}\")\n",
    "    print(f\"Predicción numérica: {pred:.4f}\")\n",
    "    print(f\"Sentimiento predicho: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f787ac",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "Entrenar una CNN con capas convolucionales para clasificación de sentimiento en tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5068b8ed",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Cargar dataset de tweets multilingüe y preparar datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128d85a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cardiffnlp/tweet_sentiment_multilingual\", \"spanish\", split='train')\n",
    "\n",
    "print(dataset.column_names)\n",
    "\n",
    "filtered_data = dataset.filter(lambda x: x['label'] in [0, 2])\n",
    "\n",
    "labels = [1 if label == 2 else 0 for label in filtered_data['label']]\n",
    "texts = filtered_data['text']\n",
    "vocab_size = 5000\n",
    "max_length = 100\n",
    "\n",
    "print(f\"Tamaño total del dataset: {len(texts)}\")\n",
    "print(f\"Distribución de etiquetas: Positivos={labels.count(1)}, Negativos={labels.count(0)}\")\n",
    "\n",
    "vocabulary_train = prepare_vocabulary(texts, vocab_size, False)\n",
    "print(\"\\nVocabulario (\", len(vocabulary_train), \"):\")\n",
    "print(vocabulary_train)\n",
    "prepared_sentences = prepare_sentences(texts, vocabulary_train, max_length, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb2471",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Configurar modelo CNN con capas convolucionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac63497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding, Conv1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "embedding_dim = 128\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, max_length))\n",
    "model.summary()\n",
    "\n",
    "print(\"Red diseñada correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed606a31",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f20278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "prepared_sentences = np.array(prepared_sentences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(prepared_sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[earlyStopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e950d8",
   "metadata": {},
   "source": [
    "### Apartado d\n",
    "Curva de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa16ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "accuracy = history_dict['accuracy']\n",
    "val_accuracy = history_dict['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax[0].plot(epochs, accuracy, 'b', label='Training accuracy')\n",
    "ax[0].plot(epochs, val_accuracy, 'red', label='Validation accuracy')\n",
    "ax[0].set_title('Training & Validation Accuracy', fontsize=16)\n",
    "ax[0].set_xlabel('Epochs', fontsize=16)\n",
    "ax[0].set_ylabel('Accuracy', fontsize=16)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(epochs, loss_values, 'b', label='Training loss')\n",
    "ax[1].plot(epochs, val_loss_values, 'red', label='Validation loss')\n",
    "ax[1].set_title('Training & Validation Loss', fontsize=16)\n",
    "ax[1].set_xlabel('Epochs', fontsize=16)\n",
    "ax[1].set_ylabel('Loss', fontsize=16)\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45755cad",
   "metadata": {},
   "source": [
    "### Apartado e\n",
    "Evaluar el modelo en test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cbe250",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b9809",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "Introducir word embeddings preentrenados (Word2Vec)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f03142",
   "metadata": {},
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(str(PATH_MODELS / 'SBW-vectors-300-min5.txt'), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "PATH_MODELS = Path.cwd().parent / 'models'\n",
    "\n",
    "w2v = KeyedVectors.load_word2vec_format(str(PATH_MODELS / 'SBW-vectors-300-min5.txt'), binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811fcd3f",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Calcular matriz de embeddings para todo el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf96107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_embedding_matrix(w2v_model, vocab, emb_dim):\n",
    "    embedding_matrix = np.zeros((len(vocab) + 1, emb_dim))\n",
    "    for token, idx in vocab.items():\n",
    "        if token in w2v_model:\n",
    "            embedding_matrix[idx] = w2v_model[token]\n",
    "        else:\n",
    "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(emb_dim,))\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = build_embedding_matrix(w2v, vocabulary_train, 300)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0895b38c",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Entrenar con la matriz de embedding preentrenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53277c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding, Conv1D, MaxPooling1D\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    input_dim=embedding_matrix.shape[0],\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=True\n",
    "))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.build(input_shape=(None, max_length))\n",
    "model.summary()\n",
    "\n",
    "print(\"Red diseñada correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7828492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "prepared_sentences = np.array(prepared_sentences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(prepared_sentences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[earlyStopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
