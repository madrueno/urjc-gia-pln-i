{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# Tema 4: Redes neuronales recurrentes para detección de noticias falsas"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Ejercicio 1 - RNN\n", "Entrenar RNNs (SimpleRNN) utilizando Keras de TensorFlow para detectar noticias falsas."]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado a\n", "Importar librerías y cargar modelos."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["import numpy as np\n", "import pandas as pd\n", "import spacy\n", "from pathlib import Path\n", "\n", "from gensim.models import KeyedVectors\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import accuracy_score\n", "from tensorflow.keras.preprocessing.sequence import pad_sequences\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Bidirectional, Dropout\n", "from tensorflow.keras.callbacks import EarlyStopping\n", "from tqdm import tqdm\n", "\n", "PATH_DATA = Path.cwd().parent / 'data'\n", "PATH_MODELS = Path.cwd().parent / 'models'"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["nlp = spacy.load('es_core_news_sm')\n", "w2v = KeyedVectors.load_word2vec_format(str(PATH_MODELS / 'SBW-vectors-300-min5.txt'), binary=False)"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado b\n", "Cargar y explorar el dataset."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["df = pd.read_excel(str(PATH_DATA / 'train.xlsx'), engine=\"openpyxl\")\n", "\n", "df.head()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["df['Category'].value_counts()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado c\n", "Análisis rápido de longitud de textos."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["(df['Headline']\n", " .apply(lambda x: len(x.split()))\n", " .describe())"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["(df['Text']\n", " .apply(lambda x: len(x.split()))\n", " .describe())"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["texts, headlines = df['Text'].tolist(), df['Headline'].tolist()\n", "labels = np.array([1 if cat == 'Fake' else 0 for cat in df['Category']])\n", "\n", "len(headlines), len(texts), len(labels)"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado d\n", "Funciones de indexación y transformación."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["def build_indexer(corpus, vocab_size):\n", "    token_to_index = {}\n", "    current_index = 1\n", "\n", "    for sentence in tqdm(corpus):\n", "        doc = nlp(sentence)\n", "        for token in doc:\n", "            clean_token = token.text.lower()\n", "            if clean_token not in token_to_index and current_index < vocab_size:\n", "                token_to_index[clean_token] = current_index\n", "                current_index += 1\n", "\n", "    print(\"\\nVocabulario (\", len(token_to_index), \"):\")\n", "    print(token_to_index)\n", "\n", "    return token_to_index\n", "\n", "\n", "def transform_text(corpus, token_to_index, max_length=10):\n", "    encoded_sentences = []\n", "    for sentence in tqdm(corpus):\n", "        doc = nlp(sentence)\n", "        encoded_sentence = []\n", "        for token in doc:\n", "            clean_token = token.text.lower()\n", "            if clean_token in token_to_index:\n", "                encoded_sentence.append(token_to_index[clean_token])\n", "            else:\n", "                encoded_sentence.append(0)\n", "        encoded_sentences.append(encoded_sentence)\n", "\n", "    prepared_sentences = pad_sequences(encoded_sentences, maxlen=max_length, padding='post', truncating='post')\n", "    print(\"Oraciones procesadas(\", len(prepared_sentences), \"):\")\n", "    print(prepared_sentences)\n", "\n", "    return prepared_sentences"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["def build_embedding_matrix(w2v_model, vocab, emb_dim):\n", "    embedding_matrix = np.zeros((len(vocab) + 1, emb_dim))\n", "    for token, idx in vocab.items():\n", "        if token in w2v_model:\n", "            embedding_matrix[idx] = w2v_model[token]\n", "        else:\n", "            embedding_matrix[idx] = np.random.normal(scale=0.6, size=(emb_dim,))\n", "    return embedding_matrix"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado e\n", "Preparar datos: separación train/validation/test."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["vocab_size_headlines = 5000\n", "\n", "X_train_headlines, X_val_headlines, y_train_headlines, y_val_headlines = train_test_split(headlines, labels, test_size=0.3, random_state=42)\n", "X_val_headlines, X_test_headlines, y_val_headlines, y_test_headlines = train_test_split(X_val_headlines, y_val_headlines, test_size=0.5, random_state=42)\n", "\n", "token_to_index_headlines = build_indexer(X_train_headlines, vocab_size=vocab_size_headlines)\n", "\n", "print(len(X_train_headlines), len(X_val_headlines), len(X_test_headlines))\n", "print(len(y_train_headlines), len(y_val_headlines), len(y_test_headlines))"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["vocab_size_texts = 20000\n", "\n", "X_train_texts, X_val_texts, y_train_texts, y_val_texts = train_test_split(texts, labels, test_size=0.3, random_state=42)\n", "X_val_texts, X_test_texts, y_val_texts, y_test_texts = train_test_split(X_val_texts, y_val_texts, test_size=0.5, random_state=42)\n", "\n", "token_to_index_texts = build_indexer(X_train_texts, vocab_size=vocab_size_texts)\n", "\n", "print(len(X_train_texts), len(X_val_texts), len(X_test_texts))\n", "print(len(y_train_texts), len(y_val_texts), len(y_test_texts))"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado f\n", "Obtener matrices de índices."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["max_length_headlines = 15\n", "\n", "X_train_headlines = transform_text(X_train_headlines, token_to_index_headlines, max_length_headlines)\n", "X_val_headlines = transform_text(X_val_headlines, token_to_index_headlines, max_length_headlines)\n", "X_test_headlines = transform_text(X_test_headlines, token_to_index_headlines, max_length_headlines)"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["max_length_texts = 500\n", "\n", "X_train_texts = transform_text(X_train_texts, token_to_index_texts, max_length_texts)\n", "X_val_texts = transform_text(X_val_texts, token_to_index_texts, max_length_texts)\n", "X_test_texts = transform_text(X_test_texts, token_to_index_texts, max_length_texts)"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado g\n", "Entrenar RNN con headlines (embeddings desde cero)."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["model = Sequential()\n", "\n", "vector_size = 128\n", "\n", "model.add(Embedding(input_dim=vocab_size_headlines, output_dim=vector_size))\n", "model.add(SimpleRNN(8, recurrent_dropout=0.5))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.build(input_shape=(None, max_length_headlines))\n", "model.summary()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["batch_size = 32\n", "epochs = 5\n", "\n", "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n", "\n", "history = model.fit(\n", "    X_train_headlines,\n", "    y_train_headlines,\n", "    validation_data=(X_val_headlines, y_val_headlines),\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    callbacks=[earlyStopping]\n", ")\n", "\n", "y_pred_headlines = model.predict(X_test_headlines)\n", "print(f\"Accuracy test: {accuracy_score(y_test_headlines, y_pred_headlines > 0.5)}\")"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado h\n", "Entrenar RNN con headlines (inicialización con Word2Vec)."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["embedding_matrix = build_embedding_matrix(w2v, token_to_index_headlines, 300)\n", "embedding_matrix.shape"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["model = Sequential()\n", "\n", "model.add(Embedding(\n", "    input_dim=embedding_matrix.shape[0],\n", "    output_dim=embedding_matrix.shape[1],\n", "    weights=[embedding_matrix],\n", "    trainable=True\n", "))\n", "model.add(SimpleRNN(8, recurrent_dropout=0.5))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.build(input_shape=(None, max_length_headlines))\n", "model.summary()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["batch_size = 32\n", "epochs = 5\n", "\n", "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n", "\n", "history = model.fit(\n", "    X_train_headlines,\n", "    y_train_headlines,\n", "    validation_data=(X_val_headlines, y_val_headlines),\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    callbacks=[earlyStopping]\n", ")\n", "\n", "y_pred_headlines = model.predict(X_test_headlines)\n", "print(f\"Accuracy test: {accuracy_score(y_test_headlines, y_pred_headlines > 0.5)}\")"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado i\n", "Entrenar RNN con textos (embeddings desde cero)."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["model = Sequential()\n", "\n", "vector_size = 128\n", "\n", "model.add(Embedding(input_dim=vocab_size_texts, output_dim=vector_size))\n", "model.add(SimpleRNN(8, recurrent_dropout=0.2))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.build(input_shape=(None, max_length_texts))\n", "model.summary()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["batch_size = 32\n", "epochs = 5\n", "\n", "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n", "\n", "history = model.fit(\n", "    X_train_texts,\n", "    y_train_texts,\n", "    validation_data=(X_val_texts, y_val_texts),\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    callbacks=[earlyStopping]\n", ")\n", "\n", "y_pred_texts = model.predict(X_test_texts)\n", "print(f\"Accuracy test: {accuracy_score(y_test_texts, y_pred_texts > 0.5)}\")"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado j\n", "Entrenar RNN con textos (inicialización con Word2Vec)."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["embedding_matrix = build_embedding_matrix(w2v, token_to_index_texts, 300)\n", "embedding_matrix.shape"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["model = Sequential()\n", "\n", "model.add(Embedding(\n", "    input_dim=embedding_matrix.shape[0],\n", "    output_dim=embedding_matrix.shape[1],\n", "    weights=[embedding_matrix],\n", "    trainable=True\n", "))\n", "model.add(SimpleRNN(8, recurrent_dropout=0.2))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.build(input_shape=(None, max_length_texts))\n", "model.summary()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["batch_size = 32\n", "epochs = 5\n", "\n", "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n", "\n", "history = model.fit(\n", "    X_train_texts,\n", "    y_train_texts,\n", "    validation_data=(X_val_texts, y_val_texts),\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    callbacks=[earlyStopping]\n", ")\n", "\n", "y_pred_texts = model.predict(X_test_texts)\n", "print(f\"Accuracy test: {accuracy_score(y_test_texts, y_pred_texts > 0.5)}\")"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Ejercicio 2 - LSTM\n", "Entrenar LSTMs utilizando Keras de TensorFlow para detectar noticias falsas."]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado a\n", "Entrenar LSTM con headlines (embeddings desde cero)."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["model = Sequential()\n", "\n", "vector_size = 128\n", "\n", "model.add(Embedding(input_dim=vocab_size_headlines, output_dim=vector_size))\n", "model.add(LSTM(8, recurrent_dropout=0.5))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.build(input_shape=(None, max_length_headlines))\n", "model.summary()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["batch_size = 32\n", "epochs = 5\n", "\n", "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n", "\n", "history = model.fit(\n", "    X_train_headlines,\n", "    y_train_headlines,\n", "    validation_data=(X_val_headlines, y_val_headlines),\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    callbacks=[earlyStopping]\n", ")\n", "\n", "y_pred_headlines = model.predict(X_test_headlines)\n", "print(f\"Accuracy test: {accuracy_score(y_test_headlines, y_pred_headlines > 0.5)}\")"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado b\n", "Entrenar LSTM con headlines (inicialización con Word2Vec)."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["embedding_matrix = build_embedding_matrix(w2v, token_to_index_headlines, 300)\n", "embedding_matrix.shape"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["model = Sequential()\n", "\n", "model.add(Embedding(\n", "    input_dim=embedding_matrix.shape[0],\n", "    output_dim=embedding_matrix.shape[1],\n", "    weights=[embedding_matrix],\n", "    trainable=True\n", "))\n", "model.add(LSTM(8, recurrent_dropout=0.5))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.build(input_shape=(None, max_length_headlines))\n", "model.summary()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["batch_size = 32\n", "epochs = 5\n", "\n", "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n", "\n", "history = model.fit(\n", "    X_train_headlines,\n", "    y_train_headlines,\n", "    validation_data=(X_val_headlines, y_val_headlines),\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    callbacks=[earlyStopping]\n", ")\n", "\n", "y_pred_headlines = model.predict(X_test_headlines)\n", "print(f\"Accuracy test: {accuracy_score(y_test_headlines, y_pred_headlines > 0.5)}\")"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado c\n", "Entrenar LSTM con textos (embeddings desde cero)."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["model = Sequential()\n", "\n", "vector_size = 128\n", "\n", "model.add(Embedding(input_dim=vocab_size_texts, output_dim=vector_size))\n", "model.add(LSTM(8, recurrent_dropout=0.5))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.build(input_shape=(None, max_length_texts))\n", "model.summary()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["batch_size = 32\n", "epochs = 5\n", "\n", "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n", "\n", "history = model.fit(\n", "    X_train_texts,\n", "    y_train_texts,\n", "    validation_data=(X_val_texts, y_val_texts),\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    callbacks=[earlyStopping]\n", ")\n", "\n", "y_pred_texts = model.predict(X_test_texts)\n", "print(f\"Accuracy test: {accuracy_score(y_test_texts, y_pred_texts > 0.5)}\")"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado d\n", "Entrenar LSTM con textos (inicialización con Word2Vec)."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["embedding_matrix = build_embedding_matrix(w2v, token_to_index_texts, 300)\n", "embedding_matrix.shape"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["model = Sequential()\n", "\n", "model.add(Embedding(\n", "    input_dim=embedding_matrix.shape[0],\n", "    output_dim=embedding_matrix.shape[1],\n", "    weights=[embedding_matrix],\n", "    trainable=True\n", "))\n", "model.add(LSTM(8, recurrent_dropout=0.5))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.build(input_shape=(None, max_length_texts))\n", "model.summary()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["batch_size = 32\n", "epochs = 5\n", "\n", "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n", "\n", "history = model.fit(\n", "    X_train_texts,\n", "    y_train_texts,\n", "    validation_data=(X_val_texts, y_val_texts),\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    callbacks=[earlyStopping]\n", ")\n", "\n", "y_pred_texts = model.predict(X_test_texts)\n", "print(f\"Accuracy test: {accuracy_score(y_test_texts, y_pred_texts > 0.5)}\")"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Ejercicio 3 - Bi-LSTM\n", "Entrenar redes neuronales recurrentes LSTM Bidireccionales."]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado a\n", "Entrenar Bi-LSTM con headlines (embeddings desde cero)."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["DROPOUT = 0.4\n", "\n", "model = Sequential()\n", "\n", "vector_size = 128\n", "\n", "model.add(Embedding(input_dim=vocab_size_headlines, output_dim=vector_size))\n", "model.add(Bidirectional(LSTM(60, return_sequences=True, recurrent_dropout=0.2)))\n", "model.add(Dropout(DROPOUT))\n", "model.add(Bidirectional(LSTM(32, recurrent_dropout=0.2)))\n", "model.add(Dropout(DROPOUT))\n", "model.add(Dense(60, activation='relu'))\n", "model.add(Dense(50, activation='relu'))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.build(input_shape=(None, max_length_headlines))\n", "model.summary()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["batch_size = 32\n", "epochs = 5\n", "\n", "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n", "\n", "history = model.fit(\n", "    X_train_headlines,\n", "    y_train_headlines,\n", "    validation_data=(X_val_headlines, y_val_headlines),\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    callbacks=[earlyStopping]\n", ")\n", "\n", "y_pred_headlines = model.predict(X_test_headlines)\n", "print(f\"Accuracy test: {accuracy_score(y_test_headlines, y_pred_headlines > 0.5)}\")"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado b\n", "Entrenar Bi-LSTM con headlines (inicialización con Word2Vec)."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["embedding_matrix = build_embedding_matrix(w2v, token_to_index_headlines, 300)"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["DROPOUT = 0.4\n", "\n", "model = Sequential()\n", "\n", "model.add(Embedding(\n", "    input_dim=embedding_matrix.shape[0],\n", "    output_dim=embedding_matrix.shape[1],\n", "    weights=[embedding_matrix],\n", "    trainable=True\n", "))\n", "model.add(Bidirectional(LSTM(60, return_sequences=True, recurrent_dropout=0.2)))\n", "model.add(Dropout(DROPOUT))\n", "model.add(Bidirectional(LSTM(32, recurrent_dropout=0.2)))\n", "model.add(Dropout(DROPOUT))\n", "model.add(Dense(60, activation='relu'))\n", "model.add(Dense(50, activation='relu'))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.build(input_shape=(None, max_length_headlines))\n", "model.summary()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["batch_size = 32\n", "epochs = 5\n", "\n", "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n", "\n", "history = model.fit(\n", "    X_train_headlines,\n", "    y_train_headlines,\n", "    validation_data=(X_val_headlines, y_val_headlines),\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    callbacks=[earlyStopping]\n", ")\n", "\n", "y_pred_headlines = model.predict(X_test_headlines)\n", "print(f\"Accuracy test: {accuracy_score(y_test_headlines, y_pred_headlines > 0.5)}\")"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado c\n", "Entrenar Bi-LSTM con textos (embeddings desde cero)."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["DROPOUT = 0.4\n", "\n", "model = Sequential()\n", "\n", "vector_size = 128\n", "\n", "model.add(Embedding(input_dim=vocab_size_texts, output_dim=vector_size))\n", "model.add(Bidirectional(LSTM(60, return_sequences=True, recurrent_dropout=0.2)))\n", "model.add(Dropout(DROPOUT))\n", "model.add(Bidirectional(LSTM(32, recurrent_dropout=0.2)))\n", "model.add(Dropout(DROPOUT))\n", "model.add(Dense(60, activation='relu'))\n", "model.add(Dense(50, activation='relu'))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.build(input_shape=(None, max_length_texts))\n", "model.summary()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["batch_size = 32\n", "epochs = 5\n", "\n", "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n", "\n", "history = model.fit(\n", "    X_train_texts,\n", "    y_train_texts,\n", "    validation_data=(X_val_texts, y_val_texts),\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    callbacks=[earlyStopping]\n", ")\n", "\n", "y_pred_texts = model.predict(X_test_texts)\n", "print(f\"Accuracy test: {accuracy_score(y_test_texts, y_pred_texts > 0.5)}\")"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["### Apartado d\n", "Entrenar Bi-LSTM con textos (inicialización con Word2Vec)."]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["embedding_matrix = build_embedding_matrix(w2v, token_to_index_texts, 300)"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["DROPOUT = 0.4\n", "\n", "model = Sequential()\n", "\n", "model.add(Embedding(\n", "    input_dim=embedding_matrix.shape[0],\n", "    output_dim=embedding_matrix.shape[1],\n", "    weights=[embedding_matrix],\n", "    trainable=True\n", "))\n", "model.add(Bidirectional(LSTM(60, return_sequences=True, recurrent_dropout=0.2)))\n", "model.add(Dropout(DROPOUT))\n", "model.add(Bidirectional(LSTM(32, recurrent_dropout=0.2)))\n", "model.add(Dropout(DROPOUT))\n", "model.add(Dense(60, activation='relu'))\n", "model.add(Dense(50, activation='relu'))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n", "model.build(input_shape=(None, max_length_texts))\n", "model.summary()"],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": ["batch_size = 32\n", "epochs = 5\n", "\n", "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n", "\n", "history = model.fit(\n", "    X_train_texts,\n", "    y_train_texts,\n", "    validation_data=(X_val_texts, y_val_texts),\n", "    batch_size=batch_size,\n", "    epochs=epochs,\n", "    callbacks=[earlyStopping]\n", ")\n", "\n", "y_pred_texts = model.predict(X_test_texts)\n", "print(f\"Accuracy test: {accuracy_score(y_test_texts, y_pred_texts > 0.5)}\")"],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}