{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f78062",
   "metadata": {},
   "source": [
    "# Tema 2: Análisis sintáctico y Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fbac3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "from pathlib import Path\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "\n",
    "PATH_DATA = Path.cwd().parent / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1903edb8",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "Análisis sintáctico parcial (chunking) de una frase en español."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b52e224",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Definir gramática para identificar chunks en español.\n",
    "\n",
    "**Nota:** NLTK no soporta PoS tagging en español, por lo que comete errores (ej: \"usa\" etiquetado como JJ en lugar de VB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e0d2f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens con PoS (español):\n",
      "[('Juan', 'NNP'), ('usa', 'JJ'), ('la', 'NN'), ('bicicleta', 'NN'), ('de', 'IN'), ('Clara', 'NNP'), ('todos', 'CC'), ('los', 'JJ'), ('días', 'JJ'), ('soleados', 'NN'), ('.', '.')]\n",
      "\n",
      "Árbol de chunks:\n",
      "(S\n",
      "  (NP Juan/NNP usa/JJ la/NN bicicleta/NN)\n",
      "  de/IN\n",
      "  Clara/NNP\n",
      "  todos/CC\n",
      "  (NP los/JJ días/JJ soleados/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "frase_es = \"Juan usa la bicicleta de Clara todos los días soleados.\"\n",
    "\n",
    "tokens_es = word_tokenize(frase_es, language='spanish')\n",
    "pos_tags_es = pos_tag(tokens_es)\n",
    "print(\"Tokens con PoS (español):\")\n",
    "print(pos_tags_es)\n",
    "\n",
    "# Gramática para identificar chunks\n",
    "grammar = r\"\"\"\n",
    "    NP: {<NNP><JJ><NN><NN>}    # Nombre propio + adjetivo + nombres\n",
    "        {<JJ><JJ><NN>}         # Adjetivos + nombre\n",
    "    PP: {<IN><NP>}             # Preposición + sintagma nominal\n",
    "\"\"\"\n",
    "\n",
    "chunk_parser = RegexpParser(grammar)\n",
    "tree = chunk_parser.parse(pos_tags_es)\n",
    "print(\"\\nÁrbol de chunks:\")\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3603092",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Traducir la frase a inglés y aplicar la misma gramática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "719c5e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens con PoS (inglés):\n",
      "[('Juan', 'JJ'), ('uses', 'VBZ'), ('Clara', 'NNP'), (\"'s\", 'POS'), ('bicycle', 'NN'), ('every', 'DT'), ('sunny', 'JJ'), ('day', 'NN'), ('.', '.')]\n",
      "\n",
      "Árbol de chunks (inglés):\n",
      "(S\n",
      "  Juan/JJ\n",
      "  uses/VBZ\n",
      "  Clara/NNP\n",
      "  's/POS\n",
      "  bicycle/NN\n",
      "  every/DT\n",
      "  sunny/JJ\n",
      "  day/NN\n",
      "  ./.)\n",
      "\n",
      "Nota: Se identifican menos chunks porque el PoS es más preciso en inglés.\n"
     ]
    }
   ],
   "source": [
    "frase_en = \"Juan uses Clara's bicycle every sunny day.\"\n",
    "\n",
    "tokens_en = word_tokenize(frase_en)\n",
    "pos_tags_en = pos_tag(tokens_en)\n",
    "print(\"Tokens con PoS (inglés):\")\n",
    "print(pos_tags_en)\n",
    "\n",
    "tree_en = chunk_parser.parse(pos_tags_en)\n",
    "print(\"\\nÁrbol de chunks (inglés):\")\n",
    "print(tree_en)\n",
    "print(\"\\nNota: Se identifican menos chunks porque el PoS es más preciso en inglés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c69b5",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "Chunking con RegexpParser para detectar diferentes sintagmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1e494d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto cargado con 6 oraciones.\n"
     ]
    }
   ],
   "source": [
    "with open(PATH_DATA / 'Cycling.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(f\"Texto cargado con {len(sentences)} oraciones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d46a66",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Chunks formados por dos nombres (NN NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c513b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oración: Cycling's world governing body, the UCI, says it h...\n",
      "(S\n",
      "  Cycling/VBG\n",
      "  's/POS\n",
      "  (NN2 world/NN governing/NN)\n",
      "  body/NN\n",
      "  ,/,\n",
      "  the/DT\n",
      "  UCI/NNP\n",
      "  ,/,\n",
      "  says/VBZ\n",
      "  it/PRP\n",
      "  has/VBZ\n",
      "  no/DT\n",
      "  plans/NNS\n",
      "  to/TO\n",
      "  move/VB\n",
      "  the/DT\n",
      "  2025/CD\n",
      "  Road/NNP\n",
      "  World/NNP\n",
      "  Championships/NNP\n",
      "  away/RB\n",
      "  from/IN\n",
      "  Rwanda/NNP\n",
      "  amid/IN\n",
      "  the/DT\n",
      "  ongoing/JJ\n",
      "  conflict/NN\n",
      "  in/IN\n",
      "  neighbouring/VBG\n",
      "  DR/NNP\n",
      "  Congo/NNP\n",
      "  ./.)\n",
      "\n",
      "Oración: Rwanda is set to become the first African nation t...\n",
      "(S\n",
      "  Rwanda/NNP\n",
      "  is/VBZ\n",
      "  set/VBN\n",
      "  to/TO\n",
      "  become/VB\n",
      "  the/DT\n",
      "  first/JJ\n",
      "  African/JJ\n",
      "  nation/NN\n",
      "  to/TO\n",
      "  host/VB\n",
      "  (DTNN the/DT event/NN)\n",
      "  from/IN\n",
      "  21-28/JJ\n",
      "  September/NNP\n",
      "  ./.)\n",
      "\n",
      "Oración: The M23 rebel group has captured almost all of the...\n",
      "(S\n",
      "  The/DT\n",
      "  M23/NNP\n",
      "  (NN2 rebel/NN group/NN)\n",
      "  has/VBZ\n",
      "  captured/VBN\n",
      "  almost/RB\n",
      "  all/DT\n",
      "  of/IN\n",
      "  the/DT\n",
      "  eastern/JJ\n",
      "  Congolese/NNP\n",
      "  city/NN\n",
      "  of/IN\n",
      "  Goma/NNP\n",
      "  and/CC\n",
      "  threatened/VBD\n",
      "  to/TO\n",
      "  continue/VB\n",
      "  its/PRP$\n",
      "  offensive/JJ\n",
      "  to/TO\n",
      "  (DTNN the/DT capital/NN)\n",
      "  ,/,\n",
      "  Kinshasa/NNP\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  is/VBZ\n",
      "  2,600km/CD\n",
      "  (/(\n",
      "  1,600/CD\n",
      "  miles/NNS\n",
      "  )/)\n",
      "  away/RB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "grammar_all = r\"\"\"\n",
    "    NN2: {<NN><NN>}              # Dos nombres\n",
    "    DTNN: {<DT><NN>}             # Determinante + nombre\n",
    "    PP: {<IN><DT>?<NN>}          # Preposición + (determinante opcional) + nombre\n",
    "    VAN: {<VB|VBZ|VBP><JJ><NN>}  # Verbo + adjetivo + nombre\n",
    "\"\"\"\n",
    "\n",
    "chunk_parser = RegexpParser(grammar_all)\n",
    "\n",
    "for sentence in sentences[:3]:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    tree = chunk_parser.parse(pos_tags)\n",
    "    print(f\"\\nOración: {sentence[:50]}...\")\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce500a6",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Extraer y mostrar todos los chunks encontrados por tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4fbef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks NN+NN: ['world governing', 'rebel group']\n",
      "\n",
      "Chunks DT+NN: ['the event', 'the capital', 'the spread', 'this subject', 'no relocation', 'this time', 'a statement']\n",
      "\n",
      "Chunks PP: ['on organisation', 'for tourism', 'of rumours']\n",
      "\n",
      "Chunks VB+JJ+NN: []\n"
     ]
    }
   ],
   "source": [
    "def extract_chunks(tree, chunk_type):\n",
    "    \"\"\"Extrae chunks de un tipo específico del árbol.\"\"\"\n",
    "    chunks = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == chunk_type:\n",
    "            chunk = ' '.join([word for word, tag in subtree.leaves()])\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "all_nn2 = []\n",
    "all_dtnn = []\n",
    "all_pp = []\n",
    "all_van = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    tree = chunk_parser.parse(pos_tags)\n",
    "    \n",
    "    all_nn2.extend(extract_chunks(tree, 'NN2'))\n",
    "    all_dtnn.extend(extract_chunks(tree, 'DTNN'))\n",
    "    all_pp.extend(extract_chunks(tree, 'PP'))\n",
    "    all_van.extend(extract_chunks(tree, 'VAN'))\n",
    "\n",
    "print(\"Chunks NN+NN:\", all_nn2[:10])\n",
    "print(\"\\nChunks DT+NN:\", all_dtnn[:10])\n",
    "print(\"\\nChunks PP:\", all_pp[:10])\n",
    "print(\"\\nChunks VB+JJ+NN:\", all_van[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32d65e",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "Reconocimiento de Entidades Nombradas (NER) con NLTK en texto en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5d591d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades nombradas encontradas:\n",
      "  ORGANIZATION: UCI\n",
      "  GSP: Rwanda\n",
      "  GSP: Rwanda\n",
      "  GPE: African\n",
      "  ORGANIZATION: M23\n",
      "  GPE: Goma\n",
      "  GPE: Kinshasa\n",
      "  ORGANIZATION: UCI\n",
      "  GPE: Kigali\n",
      "  GSP: Rwanda\n",
      "  ORGANIZATION: UCI\n",
      "  ORGANIZATION: UCI Road\n",
      "  GSP: Rwanda\n",
      "  GPE: Switzerland\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "with open(PATH_DATA / 'Cycling.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "print(\"Entidades nombradas encontradas:\")\n",
    "for sentence in sentences:\n",
    "    tokens = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    named_entities = ne_chunk(pos_tags)\n",
    "    \n",
    "    for subtree in named_entities.subtrees():\n",
    "        if subtree.label() != 'S':\n",
    "            entity = ' '.join([word for word, tag in subtree.leaves()])\n",
    "            print(f\"  {subtree.label()}: {entity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9588297",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "NER con NLTK en texto en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60dd7421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades nombradas encontradas (NLTK en español):\n",
      "  GPE: La\n",
      "  ORGANIZATION: ChatGPT\n",
      "  ORGANIZATION: DeepSeek\n",
      "  PERSON: Está\n",
      "  PERSON: Nuria Ribelles\n",
      "  PERSON: Oncología Médica\n",
      "  ORGANIZATION: Hospital Virgen\n",
      "  GPE: Victoria\n",
      "  GPE: Málaga\n",
      "  GPE: El\n",
      "  PERSON: Evaluación\n",
      "\n",
      "Nota: El resultado NO es correcto porque NLTK no soporta NER en español.\n"
     ]
    }
   ],
   "source": [
    "with open(PATH_DATA / 'Health_IA.txt', 'r', encoding='utf-8') as f:\n",
    "    text_es = f.read()\n",
    "\n",
    "sentences_es = nltk.sent_tokenize(text_es, language='spanish')\n",
    "\n",
    "print(\"Entidades nombradas encontradas (NLTK en español):\")\n",
    "for sentence in sentences_es[:3]:\n",
    "    tokens = word_tokenize(sentence, language='spanish')\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    named_entities = ne_chunk(pos_tags)\n",
    "    \n",
    "    for subtree in named_entities.subtrees():\n",
    "        if subtree.label() != 'S':\n",
    "            entity = ' '.join([word for word, tag in subtree.leaves()])\n",
    "            print(f\"  {subtree.label()}: {entity}\")\n",
    "\n",
    "print(\"\\nNota: El resultado NO es correcto porque NLTK no soporta NER en español.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d543f",
   "metadata": {},
   "source": [
    "## Ejercicio 5\n",
    "NER con spaCy en texto en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "060cfde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades nombradas encontradas (spaCy en español):\n",
      "  MISC: ChatGPT\n",
      "  MISC: DeepSeek\n",
      "  PER: Está\n",
      "  LOC: Nuria Ribelles\n",
      "  LOC: jefa de Sección de Oncología Médica del Hospital Virgen de la Victoria\n",
      "  LOC: Málaga\n",
      "  MISC: El uso de la inteligencia\n",
      "  LOC: Sección SEOM de\n",
      "  MISC: Evaluación de Resultados\n",
      "  MISC: El uso de la inteligencia\n",
      "  PER: mamografías\n",
      "  MISC: Las herramientas\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "with open(PATH_DATA / 'Health_IA.txt', 'r', encoding='utf-8') as f:\n",
    "    text_es = f.read()\n",
    "\n",
    "doc = nlp(text_es)\n",
    "\n",
    "print(\"Entidades nombradas encontradas (spaCy en español):\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"  {ent.label_}: {ent.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "963a4720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entidades agrupadas por tipo:\n",
      "\n",
      "MISC (Miscellaneous entities, e.g. events, nationalities, products or works of art):\n",
      "  ['DeepSeek', 'El uso de la inteligencia', 'Las herramientas', 'Evaluación de Resultados', 'ChatGPT']\n",
      "\n",
      "PER (Named person or family.):\n",
      "  ['mamografías', 'Está']\n",
      "\n",
      "LOC (Non-GPE locations, mountain ranges, bodies of water):\n",
      "  ['Nuria Ribelles', 'Sección SEOM de', 'Málaga', 'jefa de Sección de Oncología Médica del Hospital Virgen de la Victoria']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "entities_by_type = defaultdict(list)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    entities_by_type[ent.label_].append(ent.text)\n",
    "\n",
    "print(\"Entidades agrupadas por tipo:\")\n",
    "for label, entities in entities_by_type.items():\n",
    "    unique_entities = list(set(entities))\n",
    "    print(f\"\\n{label} ({spacy.explain(label)}):\")\n",
    "    print(f\"  {unique_entities[:10]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02-analisis-textual (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
