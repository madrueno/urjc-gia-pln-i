{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Tema 2: Análisis textual con spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "nlp_md = spacy.load('es_core_news_md')\n",
    "\n",
    "PATH_DATA = Path.cwd().parent / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio1",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "Tokenizar un texto en español y extraer información de cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ejercicio1_texto",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"Meta, la empresa antes conocida como Facebook, tendrá este año la supercomputadora más potente del mundo dedicada a tareas de inteligencia artificial. La máquina, conocida como AI Research SuperCluster, o RSC, está ya en funcionamiento, aunque no con su capacidad final de cálculo, y se empleará para generar modelos de aprendizaje máquina capaces de funcionar en todo tipo de escenarios, desde la moderación de comentarios hasta el diseño de entornos virtuales.\"\"\"\n",
    "\n",
    "doc = nlp(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio1a",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Mostrar cada token en una línea diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ejercicio1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta\n",
      ",\n",
      "la\n",
      "empresa\n",
      "antes\n",
      "conocida\n",
      "como\n",
      "Facebook\n",
      ",\n",
      "tendrá\n",
      "este\n",
      "año\n",
      "la\n",
      "supercomputadora\n",
      "más\n",
      "potente\n",
      "del\n",
      "mundo\n",
      "dedicada\n",
      "a\n",
      "tareas\n",
      "de\n",
      "inteligencia\n",
      "artificial\n",
      ".\n",
      "La\n",
      "máquina\n",
      ",\n",
      "conocida\n",
      "como\n",
      "AI\n",
      "Research\n",
      "SuperCluster\n",
      ",\n",
      "o\n",
      "RSC\n",
      ",\n",
      "está\n",
      "ya\n",
      "en\n",
      "funcionamiento\n",
      ",\n",
      "aunque\n",
      "no\n",
      "con\n",
      "su\n",
      "capacidad\n",
      "final\n",
      "de\n",
      "cálculo\n",
      ",\n",
      "y\n",
      "se\n",
      "empleará\n",
      "para\n",
      "generar\n",
      "modelos\n",
      "de\n",
      "aprendizaje\n",
      "máquina\n",
      "capaces\n",
      "de\n",
      "funcionar\n",
      "en\n",
      "todo\n",
      "tipo\n",
      "de\n",
      "escenarios\n",
      ",\n",
      "desde\n",
      "la\n",
      "moderación\n",
      "de\n",
      "comentarios\n",
      "hasta\n",
      "el\n",
      "diseño\n",
      "de\n",
      "entornos\n",
      "virtuales\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio1b",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Añadir los tokens a una lista y mostrarla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ejercicio1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Meta, ,, la, empresa, antes, conocida, como, Facebook, ,, tendrá, este, año, la, supercomputadora, más, potente, del, mundo, dedicada, a, tareas, de, inteligencia, artificial, ., La, máquina, ,, conocida, como, AI, Research, SuperCluster, ,, o, RSC, ,, está, ya, en, funcionamiento, ,, aunque, no, con, su, capacidad, final, de, cálculo, ,, y, se, empleará, para, generar, modelos, de, aprendizaje, máquina, capaces, de, funcionar, en, todo, tipo, de, escenarios, ,, desde, la, moderación, de, comentarios, hasta, el, diseño, de, entornos, virtuales, .]\n"
     ]
    }
   ],
   "source": [
    "tokens = [token for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio1c",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Mostrar para cada token: texto, categoría gramatical y lema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ejercicio1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta PROPN Meta\n",
      ", PUNCT ,\n",
      "la DET el\n",
      "empresa NOUN empresa\n",
      "antes ADV antes\n",
      "conocida ADJ conocido\n",
      "como SCONJ como\n",
      "Facebook PROPN Facebook\n",
      ", PUNCT ,\n",
      "tendrá VERB tener\n",
      "este DET este\n",
      "año NOUN año\n",
      "la DET el\n",
      "supercomputadora NOUN supercomputadora\n",
      "más ADV más\n",
      "potente ADJ potente\n",
      "del ADP del\n",
      "mundo NOUN mundo\n",
      "dedicada ADJ dedicado\n",
      "a ADP a\n",
      "tareas NOUN tarea\n",
      "de ADP de\n",
      "inteligencia NOUN inteligencia\n",
      "artificial ADJ artificial\n",
      ". PUNCT .\n",
      "La DET el\n",
      "máquina NOUN máquina\n",
      ", PUNCT ,\n",
      "conocida ADJ conocido\n",
      "como SCONJ como\n",
      "AI PROPN AI\n",
      "Research PROPN Research\n",
      "SuperCluster PROPN SuperCluster\n",
      ", PUNCT ,\n",
      "o CCONJ o\n",
      "RSC PROPN RSC\n",
      ", PUNCT ,\n",
      "está AUX estar\n",
      "ya ADV ya\n",
      "en ADP en\n",
      "funcionamiento NOUN funcionamiento\n",
      ", PUNCT ,\n",
      "aunque SCONJ aunque\n",
      "no ADV no\n",
      "con ADP con\n",
      "su DET su\n",
      "capacidad NOUN capacidad\n",
      "final ADJ final\n",
      "de ADP de\n",
      "cálculo NOUN cálculo\n",
      ", PUNCT ,\n",
      "y CCONJ y\n",
      "se PRON él\n",
      "empleará VERB emplear\n",
      "para ADP para\n",
      "generar VERB generar\n",
      "modelos NOUN modelo\n",
      "de ADP de\n",
      "aprendizaje NOUN aprendizaje\n",
      "máquina ADJ máquina\n",
      "capaces ADJ capaz\n",
      "de ADP de\n",
      "funcionar VERB funcionar\n",
      "en ADP en\n",
      "todo DET todo\n",
      "tipo NOUN tipo\n",
      "de ADP de\n",
      "escenarios NOUN escenario\n",
      ", PUNCT ,\n",
      "desde ADP desde\n",
      "la DET el\n",
      "moderación NOUN moderación\n",
      "de ADP de\n",
      "comentarios NOUN comentario\n",
      "hasta ADP hasta\n",
      "el DET el\n",
      "diseño NOUN diseño\n",
      "de ADP de\n",
      "entornos NOUN entorno\n",
      "virtuales ADJ virtual\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio1d",
   "metadata": {},
   "source": [
    "### Apartado d\n",
    "Contar las palabras vacías (stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ejercicio1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de stopwords: 36\n",
      "Número de stopwords (sin repeticiones): 26\n",
      "{'para', 'su', 'final', 'más', 'con', 'tendrá', 'no', 'todo', 'antes', 'en', 'y', 'como', 'de', 'se', 'aunque', 'del', 'ya', 'este', 'la', 'hasta', 'o', 'a', 'está', 'desde', 'el', 'La'}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "stopWords = set()\n",
    "for token in doc:\n",
    "    if token.is_stop:\n",
    "        count += 1\n",
    "        stopWords.add(token.text)\n",
    "\n",
    "print(\"Número de stopwords:\", count)\n",
    "print(\"Número de stopwords (sin repeticiones):\", len(stopWords))\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio1e",
   "metadata": {},
   "source": [
    "### Apartado e\n",
    "Mostrar los adjetivos del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ejercicio1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjetivos en el texto:\n",
      "conocida\n",
      "potente\n",
      "dedicada\n",
      "artificial\n",
      "conocida\n",
      "final\n",
      "máquina\n",
      "capaces\n",
      "virtuales\n"
     ]
    }
   ],
   "source": [
    "print(\"Adjetivos en el texto:\")\n",
    "for token in doc:\n",
    "    if token.pos_ == \"ADJ\":\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio1f",
   "metadata": {},
   "source": [
    "### Apartado f\n",
    "Mostrar los nombres del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ejercicio1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombres en el texto:\n",
      "empresa\n",
      "año\n",
      "supercomputadora\n",
      "mundo\n",
      "tareas\n",
      "inteligencia\n",
      "máquina\n",
      "funcionamiento\n",
      "capacidad\n",
      "cálculo\n",
      "modelos\n",
      "aprendizaje\n",
      "tipo\n",
      "escenarios\n",
      "moderación\n",
      "comentarios\n",
      "diseño\n",
      "entornos\n"
     ]
    }
   ],
   "source": [
    "print(\"Nombres en el texto:\")\n",
    "for token in doc:\n",
    "    if token.pos_ == \"NOUN\":\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio1g",
   "metadata": {},
   "source": [
    "### Apartado g\n",
    "Mostrar categoría gramatical con explicación usando `spacy.explain()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ejercicio1g",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta PROPN proper noun\n",
      ", PUNCT punctuation\n",
      "la DET determiner\n",
      "empresa NOUN noun\n",
      "antes ADV adverb\n",
      "conocida ADJ adjective\n",
      "como SCONJ subordinating conjunction\n",
      "Facebook PROPN proper noun\n",
      ", PUNCT punctuation\n",
      "tendrá VERB verb\n",
      "este DET determiner\n",
      "año NOUN noun\n",
      "la DET determiner\n",
      "supercomputadora NOUN noun\n",
      "más ADV adverb\n",
      "potente ADJ adjective\n",
      "del ADP adposition\n",
      "mundo NOUN noun\n",
      "dedicada ADJ adjective\n",
      "a ADP adposition\n",
      "tareas NOUN noun\n",
      "de ADP adposition\n",
      "inteligencia NOUN noun\n",
      "artificial ADJ adjective\n",
      ". PUNCT punctuation\n",
      "La DET determiner\n",
      "máquina NOUN noun\n",
      ", PUNCT punctuation\n",
      "conocida ADJ adjective\n",
      "como SCONJ subordinating conjunction\n",
      "AI PROPN proper noun\n",
      "Research PROPN proper noun\n",
      "SuperCluster PROPN proper noun\n",
      ", PUNCT punctuation\n",
      "o CCONJ coordinating conjunction\n",
      "RSC PROPN proper noun\n",
      ", PUNCT punctuation\n",
      "está AUX auxiliary\n",
      "ya ADV adverb\n",
      "en ADP adposition\n",
      "funcionamiento NOUN noun\n",
      ", PUNCT punctuation\n",
      "aunque SCONJ subordinating conjunction\n",
      "no ADV adverb\n",
      "con ADP adposition\n",
      "su DET determiner\n",
      "capacidad NOUN noun\n",
      "final ADJ adjective\n",
      "de ADP adposition\n",
      "cálculo NOUN noun\n",
      ", PUNCT punctuation\n",
      "y CCONJ coordinating conjunction\n",
      "se PRON pronoun\n",
      "empleará VERB verb\n",
      "para ADP adposition\n",
      "generar VERB verb\n",
      "modelos NOUN noun\n",
      "de ADP adposition\n",
      "aprendizaje NOUN noun\n",
      "máquina ADJ adjective\n",
      "capaces ADJ adjective\n",
      "de ADP adposition\n",
      "funcionar VERB verb\n",
      "en ADP adposition\n",
      "todo DET determiner\n",
      "tipo NOUN noun\n",
      "de ADP adposition\n",
      "escenarios NOUN noun\n",
      ", PUNCT punctuation\n",
      "desde ADP adposition\n",
      "la DET determiner\n",
      "moderación NOUN noun\n",
      "de ADP adposition\n",
      "comentarios NOUN noun\n",
      "hasta ADP adposition\n",
      "el DET determiner\n",
      "diseño NOUN noun\n",
      "de ADP adposition\n",
      "entornos NOUN noun\n",
      "virtuales ADJ adjective\n",
      ". PUNCT punctuation\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio2",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "Trabajar con oraciones del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio2a",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Imprimir el número de oraciones y cada una de ellas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ejercicio2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de oraciones: 2\n",
      "Meta, la empresa antes conocida como Facebook, tendrá este año la supercomputadora más potente del mundo dedicada a tareas de inteligencia artificial.\n",
      "Longitud: 25\n",
      "La máquina, conocida como AI Research SuperCluster, o RSC, está ya en funcionamiento, aunque no con su capacidad final de cálculo, y se empleará para generar modelos de aprendizaje máquina capaces de funcionar en todo tipo de escenarios, desde la moderación de comentarios hasta el diseño de entornos virtuales.\n",
      "Longitud: 56\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "print(\"Número de oraciones:\", len(sentences))\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print(\"Longitud:\", len(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio2b",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Para cada oración, eliminar las palabras vacías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ejercicio2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oración original:\n",
      "Meta, la empresa antes conocida como Facebook, tendrá este año la supercomputadora más potente del mundo dedicada a tareas de inteligencia artificial.\n",
      "\n",
      "Oración filtrada:\n",
      "Meta, empresa conocida Facebook, año supercomputadora potente mundo dedicada tareas inteligencia artificial.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Oración original:\n",
      "La máquina, conocida como AI Research SuperCluster, o RSC, está ya en funcionamiento, aunque no con su capacidad final de cálculo, y se empleará para generar modelos de aprendizaje máquina capaces de funcionar en todo tipo de escenarios, desde la moderación de comentarios hasta el diseño de entornos virtuales.\n",
      "\n",
      "Oración filtrada:\n",
      "máquina, conocida AI Research SuperCluster, RSC, funcionamiento, capacidad cálculo, empleará generar modelos aprendizaje máquina capaces funcionar tipo escenarios, moderación comentarios diseño entornos virtuales.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(f\"Oración original:\\n{sent}\\n\")\n",
    "\n",
    "    # Keep original spacing/punctuation automatically\n",
    "    filtered_sentence = ''.join(token.text_with_ws for token in sent if not token.is_stop).strip()\n",
    "\n",
    "    print(f\"Oración filtrada:\\n{filtered_sentence}\\n\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio3",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "Calcular frecuencias de palabras del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio3a",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Obtener la frecuencia de cada palabra (sin stop words ni puntuación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ejercicio3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras: ['Meta', 'empresa', 'conocida', 'Facebook', 'año', 'supercomputadora', 'potente', 'mundo', 'dedicada', 'tareas', 'inteligencia', 'artificial', 'máquina', 'conocida', 'AI', 'Research', 'SuperCluster', 'RSC', 'funcionamiento', 'capacidad', 'cálculo', 'empleará', 'generar', 'modelos', 'aprendizaje', 'máquina', 'capaces', 'funcionar', 'tipo', 'escenarios', 'moderación', 'comentarios', 'diseño', 'entornos', 'virtuales']\n",
      "Frecuencias: [1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Pares palabra-frecuencia: [('Meta', 1), ('empresa', 1), ('conocida', 2), ('Facebook', 1), ('año', 1), ('supercomputadora', 1), ('potente', 1), ('mundo', 1), ('dedicada', 1), ('tareas', 1), ('inteligencia', 1), ('artificial', 1), ('máquina', 2), ('conocida', 2), ('AI', 1), ('Research', 1), ('SuperCluster', 1), ('RSC', 1), ('funcionamiento', 1), ('capacidad', 1), ('cálculo', 1), ('empleará', 1), ('generar', 1), ('modelos', 1), ('aprendizaje', 1), ('máquina', 2), ('capaces', 1), ('funcionar', 1), ('tipo', 1), ('escenarios', 1), ('moderación', 1), ('comentarios', 1), ('diseño', 1), ('entornos', 1), ('virtuales', 1)]\n",
      "Pares únicos: 33\n"
     ]
    }
   ],
   "source": [
    "words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "frecuencias = [words.count(w) for w in words]\n",
    "\n",
    "print(\"Palabras:\", words)\n",
    "print(\"Frecuencias:\", frecuencias)\n",
    "\n",
    "pairList = list(zip(words, frecuencias))\n",
    "print(\"Pares palabra-frecuencia:\", pairList)\n",
    "print(\"Pares únicos:\", len(set(pairList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio3b",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Mostrar palabras con frecuencia mayor que 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ejercicio3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras con frecuencia superior a 1: ['conocida', 'máquina']\n"
     ]
    }
   ],
   "source": [
    "setPairList = set(pairList)\n",
    "unique_words = [word for (word, freq) in setPairList if freq > 1]\n",
    "print(\"Palabras con frecuencia superior a 1:\", unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio4",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "Implementar una función de preprocesamiento de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ejercicio4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meta', 'empresa', 'conocido', 'facebook', 'año', 'supercomputadora', 'potente', 'mundo', 'dedicado', 'tarea', 'inteligencia', 'artificial', 'máquina', 'conocido', 'ai', 'research', 'supercluster', 'rsc', 'funcionamiento', 'capacidad', 'cálculo', 'emplear', 'generar', 'modelo', 'aprendizaje', 'máquina', 'capaz', 'funcionar', 'tipo', 'escenario', 'moderación', 'comentario', 'diseño', 'entorno', 'virtual']\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords_punctuation(doc):\n",
    "    return [token for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    return [token.lemma_.lower() for token in tokens]\n",
    "\n",
    "def process_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = remove_stopwords_punctuation(doc)\n",
    "    return lemmatize(tokens)\n",
    "\n",
    "print(process_text(texto))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio5",
   "metadata": {},
   "source": [
    "## Ejercicio 5\n",
    "Calcular similitud entre documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ejercicio5_textos",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_DATA / 'openAustralia1.txt', 'r', encoding=\"utf8\") as f:\n",
    "    text1 = f.read()\n",
    "\n",
    "with open(PATH_DATA / 'openAustralia2.txt', 'r', encoding=\"utf8\") as f:\n",
    "    text2 = f.read()\n",
    "\n",
    "with open(PATH_DATA / 'crisisUcrania.txt', 'r', encoding=\"utf8\") as f:\n",
    "    text3 = f.read()\n",
    "\n",
    "doc1 = nlp_md(text1)\n",
    "doc2 = nlp_md(text2)\n",
    "doc3 = nlp_md(text3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio5a",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Similitud sin preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ejercicio5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud sin preprocesamiento:\n",
      "doc1-doc2: 0.8897\n",
      "doc1-doc3: 0.8817\n",
      "doc2-doc3: 0.7522\n"
     ]
    }
   ],
   "source": [
    "print(\"Similitud sin preprocesamiento:\")\n",
    "print(f\"doc1-doc2: {doc1.similarity(doc2):.4f}\")\n",
    "print(f\"doc1-doc3: {doc1.similarity(doc3):.4f}\")\n",
    "print(f\"doc2-doc3: {doc2.similarity(doc3):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio5b",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Similitud con filtrado (sin puntuación, espacios, stop words ni tokens cortos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74b405c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_filtered(token):\n",
    "    return not (token.is_punct | token.is_space | token.is_stop | len(token.text) < 4)\n",
    "\n",
    "def spacy_processing(doc, filtering, lematization):\n",
    "    if filtering and lematization:\n",
    "        tokens = [token.lemma_ for token in doc if token_filtered(token)]\n",
    "    elif lematization:\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "    elif filtering:\n",
    "        tokens = [token.text for token in doc if token_filtered(token)]\n",
    "    else:\n",
    "        tokens = [token.text for token in doc]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ejercicio5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud con filtrado:\n",
      "doc1-doc2: 0.9234\n",
      "doc1-doc3: 0.6596\n",
      "doc2-doc3: 0.5495\n"
     ]
    }
   ],
   "source": [
    "new_doc1 = nlp_md(spacy_processing(doc1, True, False))\n",
    "new_doc2 = nlp_md(spacy_processing(doc2, True, False))\n",
    "new_doc3 = nlp_md(spacy_processing(doc3, True, False))\n",
    "\n",
    "print(\"Similitud con filtrado:\")\n",
    "print(f\"doc1-doc2: {new_doc1.similarity(new_doc2):.4f}\")\n",
    "print(f\"doc1-doc3: {new_doc1.similarity(new_doc3):.4f}\")\n",
    "print(f\"doc2-doc3: {new_doc2.similarity(new_doc3):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio5c",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Similitud con filtrado y lematización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ejercicio5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud con filtrado y lematización:\n",
      "doc1-doc2: 0.9372\n",
      "doc1-doc3: 0.7290\n",
      "doc2-doc3: 0.6333\n"
     ]
    }
   ],
   "source": [
    "new_doc1 = nlp_md(spacy_processing(doc1, True, True))\n",
    "new_doc2 = nlp_md(spacy_processing(doc2, True, True))\n",
    "new_doc3 = nlp_md(spacy_processing(doc3, True, True))\n",
    "\n",
    "print(\"Similitud con filtrado y lematización:\")\n",
    "print(f\"doc1-doc2: {new_doc1.similarity(new_doc2):.4f}\")\n",
    "print(f\"doc1-doc3: {new_doc1.similarity(new_doc3):.4f}\")\n",
    "print(f\"doc2-doc3: {new_doc2.similarity(new_doc3):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio6",
   "metadata": {},
   "source": [
    "## Ejercicio 6\n",
    "Extraer información usando el Matcher de spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ejercicio6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "with open(PATH_DATA / 'atp.txt', 'r', encoding=\"utf8\") as f:\n",
    "    sample = f.read()\n",
    "\n",
    "doc = nlp(sample)\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio6a",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Patrón para obtener la cadena \"(ATP)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ejercicio6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['(ATP)']\n"
     ]
    }
   ],
   "source": [
    "pattern = [{\"IS_PUNCT\": True}, {\"TEXT\": \"ATP\"}, {\"IS_PUNCT\": True}]\n",
    "matcher.add(\"ATP\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "print('Matches:', [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15dpv1f3h",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Patrón para obtener la cadena \"Súper challengers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fykwejptria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['Súper challengers']\n"
     ]
    }
   ],
   "source": [
    "# Crear nuevo matcher para este patrón\n",
    "matcher_super = Matcher(nlp.vocab)\n",
    "pattern = [{\"TEXT\": \"Súper\"}, {\"TEXT\": \"challengers\"}]\n",
    "matcher_super.add(\"SuperChallengers\", [pattern])\n",
    "\n",
    "matches = matcher_super(doc)\n",
    "print('Matches:', [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jlvpk8a24e",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Patrón para obtener secuencias de sustantivo seguido de adjetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5qowaxxnh3v",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['torneos gobernados', 'jugadores emergentes', 'ránking inferior', 'categoría menor', 'días previo', 'días previos', 'días previos', 'torneos conocidos', 'status especial', 'torneos grandes', 'jugadores eliminados', 'instancias iniciales', 'torneos grandes', 'veces mejores']\n"
     ]
    }
   ],
   "source": [
    "matcher_noun_adj = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\"}]\n",
    "matcher_noun_adj.add(\"NounAdj\", [pattern])\n",
    "\n",
    "matches = matcher_noun_adj(doc)\n",
    "print('Matches:', [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio7",
   "metadata": {},
   "source": [
    "## Ejercicio 7\n",
    "Extraer patrones de la colección completa de textos en español."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kxuvullg4j",
   "metadata": {},
   "source": [
    "### Apartado a\n",
    "Extraer secuencias de sustantivo seguido de adjetivo en todos los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06675966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    with open(file, 'r', encoding=\"utf8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "files = [PATH_DATA / 'atp.txt', PATH_DATA / 'openAustralia1.txt', PATH_DATA / 'openAustralia2.txt', PATH_DATA / 'crisisUcrania.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bqh8qv6cp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: atp.txt\n",
      "Matches: ['torneos gobernados', 'jugadores emergentes', 'ránking inferior', 'categoría menor', 'días previo', 'días previos', 'días previos', 'torneos conocidos', 'status especial', 'torneos grandes', 'jugadores eliminados', 'instancias iniciales', 'torneos grandes', 'veces mejores']\n",
      "\n",
      "Archivo: openAustralia1.txt\n",
      "Matches: ['sofocón final', 'vigesimoprimer major']\n",
      "\n",
      "Archivo: openAustralia2.txt\n",
      "Matches: ['achuchón sufrido', 'arreón final', 'pista dura', 'empate histórico']\n",
      "\n",
      "Archivo: crisisUcrania.txt\n",
      "Matches: ['punto muerto', 'autoproclamadas repúblicas', 'línea firme', 'mesa redonda', 'medios rusos', 'regiones separatistas']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\"}]\n",
    "matcher.add(\"NounAdj\", [pattern])\n",
    "\n",
    "for f in files:\n",
    "    text = read_file(f)\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    print(f\"Archivo: {f.name}\")\n",
    "    print('Matches:', [doc[start:end].text for match_id, start, end in matches])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x66q6n74bvt",
   "metadata": {},
   "source": [
    "### Apartado b\n",
    "Extraer secuencias de número seguido de sustantivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "z0h38y25m8o",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: atp.txt\n",
      "Matches: ['mil dólares', '21 días', '21 días', '21 días', '2 semanas']\n",
      "\n",
      "Archivo: openAustralia1.txt\n",
      "Matches: ['dos horas', '55 minutos', 'cinco títulos']\n",
      "\n",
      "Archivo: openAustralia2.txt\n",
      "Matches: ['500 días', 'tres jornadas', '55m.', '500 victorias', '20 majors', '30m']\n",
      "\n",
      "Archivo: crisisUcrania.txt\n",
      "Matches: ['siete años', 'nueve días']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"NUM\"}, {\"POS\": \"NOUN\"}]\n",
    "matcher.add(\"NumNoun\", [pattern])\n",
    "\n",
    "for f in files:\n",
    "    text = read_file(f)\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    print(f\"Archivo: {f.name}\")\n",
    "    print('Matches:', [doc[start:end].text for match_id, start, end in matches])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "om9fydt30l8",
   "metadata": {},
   "source": [
    "### Apartado c\n",
    "Extraer secuencias de nombre propio seguido de adjetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74ruon4sanw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: atp.txt\n",
      "Matches: ['Jugadores rankeados', 'Card otorgado', 'Jugadores rankeados']\n",
      "\n",
      "Archivo: openAustralia1.txt\n",
      "Matches: []\n",
      "\n",
      "Archivo: openAustralia2.txt\n",
      "Matches: []\n",
      "\n",
      "Archivo: crisisUcrania.txt\n",
      "Matches: ['Gobierno ruso', 'Exteriores ruso']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"ADJ\"}]\n",
    "matcher.add(\"PropnAdj\", [pattern])\n",
    "\n",
    "for f in files:\n",
    "    text = read_file(f)\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    print(f\"Archivo: {f.name}\")\n",
    "    print('Matches:', [doc[start:end].text for match_id, start, end in matches])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mw5281ztt9r",
   "metadata": {},
   "source": [
    "### Apartado d\n",
    "Extraer todas las cadenas que contengan el lema \"tener\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "quopislyjvl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: atp.txt\n",
      "Matches: ['tienen', 'tienen']\n",
      "\n",
      "Archivo: openAustralia1.txt\n",
      "Matches: []\n",
      "\n",
      "Archivo: openAustralia2.txt\n",
      "Matches: []\n",
      "\n",
      "Archivo: crisisUcrania.txt\n",
      "Matches: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LEMMA\": \"tener\"}]\n",
    "matcher.add(\"Tener\", [pattern])\n",
    "\n",
    "for f in files:\n",
    "    text = read_file(f)\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    print(f\"Archivo: {f.name}\")\n",
    "    print('Matches:', [doc[start:end].text for match_id, start, end in matches])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1hwfy5ez7ql",
   "metadata": {},
   "source": [
    "### Apartado e\n",
    "Usar PhraseMatcher para buscar frases exactas en todos los documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "vh1o34qp6nd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: atp.txt\n",
      "Matches: ['Grand Slam']\n",
      "Archivo: openAustralia1.txt\n",
      "Matches: ['Grand Slam']\n",
      "Archivo: openAustralia2.txt\n",
      "Matches: ['Grand Slam']\n",
      "Archivo: crisisUcrania.txt\n",
      "Matches: []\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "phrases = [\"actividad física\", \"cuerpo saludable\", \"gobierno federal\", \"Grand Slam\"]\n",
    "patterns = [nlp.make_doc(text) for text in phrases]\n",
    "matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "def matching(matcher, doc):\n",
    "    matches = matcher(doc)\n",
    "    print('Matches:', [doc[start:end].text for match_id, start, end in matches])\n",
    "\n",
    "for f in files:\n",
    "    text = read_file(f)\n",
    "    doc = nlp(text)\n",
    "    print(f\"Archivo: {f.name}\")\n",
    "    matching(matcher, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_ejercicio8",
   "metadata": {},
   "source": [
    "## Ejercicio 8\n",
    "Combinar patrones con expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ejercicio8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: atp.txt\n",
      "Matches: []\n",
      "Archivo: openAustralia1.txt\n",
      "Matches: []\n",
      "Archivo: openAustralia2.txt\n",
      "Matches: ['achuchón sufrido', 'arreón final']\n",
      "Archivo: crisisUcrania.txt\n",
      "Matches: ['autoproclamadas repúblicas']\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Buscar texto que empiece por 'a' seguido de un adjetivo\n",
    "pattern = [{\"TEXT\": {\"REGEX\": \"^a\"}}, {\"POS\": \"ADJ\"}]\n",
    "matcher.add(\"RegEx\", [pattern])\n",
    "\n",
    "for f in files:\n",
    "    text = read_file(f)\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    print(f\"Archivo: {f.name}\")\n",
    "    print('Matches:', [doc[start:end].text for match_id, start, end in matches])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02-analisis-textual (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
