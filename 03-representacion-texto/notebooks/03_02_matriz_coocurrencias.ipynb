{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41cd5f6a",
   "metadata": {},
   "source": [
    "# Tema 3: Matriz coocurrencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abc8190-6527-45df-aba6-85d8ec39fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "#nltk.download(\"punkt\")\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2be1b6",
   "metadata": {},
   "source": [
    "## Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18629d91-286f-4403-98dc-a261b2aaba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto de ejemplo\n",
    "text = \"Mysterious tunnels sketched by Leonardo da Vinci in the late 1400s may have been found at the Castle. Secret tunnels at the Sforza Castle.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e2e4c7-a592-47ca-809b-9c7cb9bf4090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se preprocesa el texto, eliminando stopwords, convirtiendo las palabras a minúsculas, tokenizando y eliminando los tokens no alfanuméricos\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "words = word_tokenize(text.lower())\n",
    "words = [word for word in words if word.isalnum() and word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0beaaa09-a778-42a3-a9f1-7f2f9e0a6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define el tamaño de ventana y se crea la lista de pares que coocurren dentro de la ventana\n",
    "\n",
    "# Definir el tamaño de ventana para la coocurrencia\n",
    "window_size = 2\n",
    "\n",
    "# Crear una lista de pares de palabras que coocurren\n",
    "co_occurrences = defaultdict(Counter)\n",
    "for i, word in enumerate(words):\n",
    "    for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
    "        if i != j:\n",
    "            co_occurrences[word][words[j]] += 1\n",
    "            # print(\"palabra:  \",word, \"  words[j]:  \", words[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b85c43c-1b14-444b-bf29-cc3970485566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se obtiene el vocabulario\n",
    "# Crear una lista de palabras únicas\n",
    "unique_words = sorted(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "429aa287-0ae2-4161-ae93-69eaa43dce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la matriz de coocurrencia\n",
    "# Inicializar la matriz de coocurrencias\n",
    "co_matrix = np.zeros((len(unique_words), len(unique_words)), dtype=int)\n",
    "\n",
    "# Poblar la matriz de coocurrencias\n",
    "word_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "for word, neighbors in co_occurrences.items():\n",
    "    for neighbor, count in neighbors.items():\n",
    "        co_matrix[word_index[word]][word_index[neighbor]] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "622418c8-92a4-45cc-ae74-7d1e87b0703a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1400s</th>\n",
       "      <th>castle</th>\n",
       "      <th>da</th>\n",
       "      <th>found</th>\n",
       "      <th>late</th>\n",
       "      <th>leonardo</th>\n",
       "      <th>may</th>\n",
       "      <th>mysterious</th>\n",
       "      <th>secret</th>\n",
       "      <th>sforza</th>\n",
       "      <th>sketched</th>\n",
       "      <th>tunnels</th>\n",
       "      <th>vinci</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1400s</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>castle</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>found</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>late</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leonardo</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>may</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mysterious</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>secret</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sforza</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sketched</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tunnels</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vinci</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1400s  castle  da  found  late  leonardo  may  mysterious  secret  \\\n",
       "1400s           0       0   0      1     1         0    1           0       0   \n",
       "castle          0       0   0      1     0         0    1           0       1   \n",
       "da              0       0   0      0     1         1    0           0       0   \n",
       "found           1       1   0      0     0         0    1           0       1   \n",
       "late            1       0   1      0     0         0    1           0       0   \n",
       "leonardo        0       0   1      0     0         0    0           0       0   \n",
       "may             1       1   0      1     1         0    0           0       0   \n",
       "mysterious      0       0   0      0     0         0    0           0       0   \n",
       "secret          0       1   0      1     0         0    0           0       0   \n",
       "sforza          0       1   0      0     0         0    0           0       1   \n",
       "sketched        0       0   1      0     0         1    0           1       0   \n",
       "tunnels         0       2   0      0     0         1    0           1       1   \n",
       "vinci           1       0   1      0     1         1    0           0       0   \n",
       "\n",
       "            sforza  sketched  tunnels  vinci  \n",
       "1400s            0         0        0      1  \n",
       "castle           1         0        2      0  \n",
       "da               0         1        0      1  \n",
       "found            0         0        0      0  \n",
       "late             0         0        0      1  \n",
       "leonardo         0         1        1      1  \n",
       "may              0         0        0      0  \n",
       "mysterious       0         1        1      0  \n",
       "secret           1         0        1      0  \n",
       "sforza           0         0        1      0  \n",
       "sketched         0         0        1      0  \n",
       "tunnels          1         1        0      0  \n",
       "vinci            0         0        0      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se crea un DataFrame para una mejor visualización\n",
    "# Crear un DataFrame para una mejor legibilidad\n",
    "co_matrix_df = pd.DataFrame(co_matrix, index=unique_words, columns=unique_words)\n",
    "\n",
    "# Mostrar la matriz de coocurrencias\n",
    "co_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_58b99f27-6951-4dcd-86bc-28332ec5fd78",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "Se quiere calcular la similitud coseno entre pares de palabras a partir de la matriz de coocurrencias anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58b99f27-6951-4dcd-86bc-28332ec5fd78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similitud del coseno entre las dos palabras es: 0.5\n",
      "La similitud del coseno entre las dos palabras es: 0.3333333333333333\n",
      "La similitud del coseno entre las dos palabras es: 1.0000000000000002\n",
      "La similitud del coseno entre las dos palabras es: 0.2357022603955158\n",
      "La similitud del coseno entre las dos palabras es: 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity_sklearn(matrix, word1_index, word2_index):\n",
    "    # Obtener los vectores de las dos palabras\n",
    "    vector1 = matrix[word1_index].reshape(1, -1)\n",
    "    vector2 = matrix[word2_index].reshape(1, -1)\n",
    "\n",
    "    # Calcular la similitud del coseno usando sklearn\n",
    "    similarity = cosine_similarity(vector1, vector2)[0][0]\n",
    "\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Calcular la similitud del coseno entre las palabras \"da\" y \"vinci\"\n",
    "similarity = cosine_similarity_sklearn(co_matrix, word_index[\"da\"], word_index[\"vinci\"])\n",
    "\n",
    "print(f\"La similitud del coseno entre las dos palabras es: {similarity}\")\n",
    "\n",
    "# Calcular la similitud del coseno entre las palabras \"may\" y \"tunnels\"\n",
    "similarity = cosine_similarity_sklearn(co_matrix, word_index[\"may\"], word_index[\"tunnels\"])\n",
    "\n",
    "print(f\"La similitud del coseno entre las dos palabras es: {similarity}\")\n",
    "\n",
    "# Calcular la similitud del coseno entre una palabra consigo misma\n",
    "similarity = cosine_similarity_sklearn(co_matrix, word_index[\"tunnels\"], word_index[\"tunnels\"])\n",
    "\n",
    "print(f\"La similitud del coseno entre las dos palabras es: {similarity}\")\n",
    "\n",
    "# Calcular la similitud del coseno entre las palabras \"tunnels\" y \"castle\"\n",
    "similarity = cosine_similarity_sklearn(co_matrix, word_index[\"tunnels\"], word_index[\"castle\"])\n",
    "\n",
    "print(f\"La similitud del coseno entre las dos palabras es: {similarity}\")\n",
    "\n",
    "# Calcular la similitud del coseno entre las palabras \"secret\" y \"tunnels\"\n",
    "similarity = cosine_similarity_sklearn(co_matrix, word_index[\"secret\"], word_index[\"tunnels\"])\n",
    "\n",
    "print(f\"La similitud del coseno entre las dos palabras es: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e6c6e2",
   "metadata": {},
   "source": [
    "## Ejercicio 3\n",
    "Se quiere calcular las palabras más similares a una dada a partir de la matriz de coocurrencias anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2e5ff27-1e5f-41c6-b011-cc7fe0482231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las palabras más similares a  leonardo  son: {similar_words_indices}\n",
      "mysterious\n",
      "late\n",
      "da\n",
      "sketched\n",
      "castle\n",
      "Las palabras más similares a  da  son: {similar_words_indices}\n",
      "leonardo\n",
      "1400s\n",
      "vinci\n",
      "mysterious\n",
      "tunnels\n"
     ]
    }
   ],
   "source": [
    "def most_similar_words(matrix, word_index, n):\n",
    "    # Obtener el vector de la palabra dada\n",
    "    word_vector = matrix[word_index].reshape(1, -1)\n",
    "\n",
    "    # Calcular la similitud del coseno entre la palabra dada y todas las demás palabras\n",
    "    similarities = cosine_similarity(matrix, word_vector).flatten()\n",
    "\n",
    "    # Obtener los índices de las n palabras más similares (excluyendo la palabra dada)\n",
    "    most_similar_indices = similarities.argsort()[-n - 1 : -1][::-1]\n",
    "\n",
    "    return most_similar_indices\n",
    "\n",
    "\n",
    "n_words_most_similar = 5\n",
    "\n",
    "# Obtener los índices de las 3 palabras más similares a la palabra en el índice que corresponda\n",
    "\n",
    "word = \"leonardo\"\n",
    "similar_words_indices = most_similar_words(co_matrix, word_index[word], n_words_most_similar)\n",
    "\n",
    "print(\"Las palabras más similares a \", word, \" son: {similar_words_indices}\")\n",
    "\n",
    "for index in similar_words_indices:\n",
    "    print(unique_words[index])\n",
    "\n",
    "\n",
    "word = \"da\"\n",
    "similar_words_indices = most_similar_words(co_matrix, word_index[word], n_words_most_similar)\n",
    "\n",
    "print(\"Las palabras más similares a \", word, \" son: {similar_words_indices}\")\n",
    "\n",
    "for index in similar_words_indices:\n",
    "    print(unique_words[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md_4bd0b42a-c216-4139-b046-efee4e6d428f",
   "metadata": {},
   "source": [
    "## Ejercicio 4\n",
    "\n",
    "Se quiere calcular las palabras más similares a una dada utilizando como pesado la función PPMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd0b42a-c216-4139-b046-efee4e6d428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de coocurrencias:\n",
      "[[0 0 0 1 1 0 1 0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0 1 0 1 1 0 2 0]\n",
      " [0 0 0 0 1 1 0 0 0 0 1 0 1]\n",
      " [1 1 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [1 0 1 0 0 0 1 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 0 0 0 1 1 1]\n",
      " [1 1 0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 1 0]\n",
      " [0 1 0 1 0 0 0 0 0 1 0 1 0]\n",
      " [0 1 0 0 0 0 0 0 1 0 0 1 0]\n",
      " [0 0 1 0 0 1 0 1 0 0 0 1 0]\n",
      " [0 2 0 0 0 1 0 1 1 1 1 0 0]\n",
      " [1 0 1 0 1 1 0 0 0 0 0 0 0]]\n",
      "\n",
      "Matriz PPMI:\n",
      "[[0.         0.         0.         1.7548875  1.7548875  0.\n",
      "  1.7548875  0.         0.         0.         0.         0.\n",
      "  1.7548875 ]\n",
      " [0.         0.         0.         1.169925   0.         0.\n",
      "  1.169925   0.         1.169925   1.5849625  0.         1.36257008\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         1.7548875  1.7548875\n",
      "  0.         0.         0.         0.         1.7548875  0.\n",
      "  1.7548875 ]\n",
      " [1.7548875  1.169925   0.         0.         0.         0.\n",
      "  1.7548875  0.         1.7548875  0.         0.         0.\n",
      "  0.        ]\n",
      " [1.7548875  0.         1.7548875  0.         0.         0.\n",
      "  1.7548875  0.         0.         0.         0.         0.\n",
      "  1.7548875 ]\n",
      " [0.         0.         1.7548875  0.         0.         0.\n",
      "  0.         0.         0.         0.         1.7548875  0.94753258\n",
      "  1.7548875 ]\n",
      " [1.7548875  1.169925   0.         1.7548875  1.7548875  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         2.7548875  1.94753258\n",
      "  0.        ]\n",
      " [0.         1.169925   0.         1.7548875  0.         0.\n",
      "  0.         0.         0.         2.169925   0.         0.94753258\n",
      "  0.        ]\n",
      " [0.         1.5849625  0.         0.         0.         0.\n",
      "  0.         0.         2.169925   0.         0.         1.36257008\n",
      "  0.        ]\n",
      " [0.         0.         1.7548875  0.         0.         1.7548875\n",
      "  0.         2.7548875  0.         0.         0.         0.94753258\n",
      "  0.        ]\n",
      " [0.         1.36257008 0.         0.         0.         0.94753258\n",
      "  0.         1.94753258 0.94753258 1.36257008 0.94753258 0.\n",
      "  0.        ]\n",
      " [1.7548875  0.         1.7548875  0.         1.7548875  1.7548875\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n",
      "Las palabras más similares a  leonardo  son: {similar_words_indices}\n",
      "mysterious\n",
      "late\n",
      "da\n",
      "sketched\n",
      "1400s\n",
      "Las palabras más similares a  da  son: {similar_words_indices}\n",
      "leonardo\n",
      "1400s\n",
      "vinci\n",
      "mysterious\n",
      "tunnels\n"
     ]
    }
   ],
   "source": [
    "def create_ppmi_matrix(cooccurrence_matrix):\n",
    "    # Calcular la suma total de la matriz de coocurrencias\n",
    "    total_sum = np.sum(cooccurrence_matrix)\n",
    "\n",
    "    # Calcular la suma de cada fila y cada columna\n",
    "    row_sums = np.sum(cooccurrence_matrix, axis=1)\n",
    "    col_sums = np.sum(cooccurrence_matrix, axis=0)\n",
    "\n",
    "    # Inicializar la matriz PPMI con ceros\n",
    "    ppmi_matrix = np.zeros_like(cooccurrence_matrix, dtype=float)\n",
    "\n",
    "    # Calcular los valores PPMI para cada celda en la matriz\n",
    "    for i in range(cooccurrence_matrix.shape[0]):\n",
    "        for j in range(cooccurrence_matrix.shape[1]):\n",
    "            p_ij = cooccurrence_matrix[i, j] / total_sum\n",
    "            p_i = row_sums[i] / total_sum\n",
    "            p_j = col_sums[j] / total_sum\n",
    "\n",
    "            if p_ij > 0:\n",
    "                ppmi_value = max(0, np.log2(p_ij / (p_i * p_j)))\n",
    "                ppmi_matrix[i, j] = ppmi_value\n",
    "\n",
    "    return ppmi_matrix\n",
    "\n",
    "\n",
    "# Crear la matriz PPMI\n",
    "ppmi_matrix = create_ppmi_matrix(co_matrix)\n",
    "\n",
    "print(\"Matriz de coocurrencias:\")\n",
    "print(co_matrix)\n",
    "\n",
    "print(\"\\nMatriz PPMI:\")\n",
    "print(ppmi_matrix)\n",
    "\n",
    "n_words_most_similar = 5\n",
    "\n",
    "# Obtener los índices de las 3 palabras más similares a la palabra en el índice que corresponda\n",
    "\n",
    "word = \"leonardo\"\n",
    "similar_words_indices = most_similar_words(ppmi_matrix, word_index[word], n_words_most_similar)\n",
    "\n",
    "print(\"Las palabras más similares a \", word, \" son: {similar_words_indices}\")\n",
    "\n",
    "for index in similar_words_indices:\n",
    "    print(unique_words[index])\n",
    "\n",
    "\n",
    "word = \"da\"\n",
    "similar_words_indices = most_similar_words(ppmi_matrix, word_index[word], n_words_most_similar)\n",
    "\n",
    "print(\"Las palabras más similares a \", word, \" son: {similar_words_indices}\")\n",
    "\n",
    "for index in similar_words_indices:\n",
    "    print(unique_words[index])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tema-03-representacion-texto (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
