{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tema 3: Embeddings Estáticos: Extensiones de Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1: Sense2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sense2vec import Sense2Vec\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar rutas\n",
    "PATH_MODELS = Path.cwd().parent / 'models'\n",
    "\n",
    "s2v = Sense2Vec().from_disk(str(PATH_MODELS / \"s2v_old\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watch as a NOUN: [-0.03000872 -0.07183316 -0.31021914  0.00982919 -0.4163286 ] ...\n",
      "Watch as a VERB: [-0.18492278  0.05598612 -0.22331624 -0.15254268 -0.2843019 ] ...\n"
     ]
    }
   ],
   "source": [
    "print('Watch as a NOUN:', s2v[\"watch|NOUN\"][:5], '...')\n",
    "print('Watch as a VERB:', s2v[\"watch|VERB\"][:5], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1.2 y 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import spacy\n",
    "\n",
    "\n",
    "w2v = api.load(\"word2vec-google-news-300\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "You must watch the light movie.\n",
      "------------------------\n",
      "watch|VERB\n",
      "\t-> Sense2Vec: [('watching|VERB', np.float32(0.8933)), ('watching|NOUN', np.float32(0.8348)), ('watched|VERB', np.float32(0.823))]\n",
      "\t-> Word2Vec: [('watching', 0.7835854291915894), ('watched', 0.6677262783050537), ('Watching', 0.6385796666145325)]\n",
      "light|ADJ\n",
      "\t-> Sense2Vec: [('dark|ADJ', np.float32(0.8024)), ('bright|ADJ', np.float32(0.7952)), ('light|NOUN', np.float32(0.7695))]\n",
      "\t-> Word2Vec: [('lights', 0.550593912601471), ('yellowish_glow', 0.5484952926635742), ('illumination', 0.5342711806297302)]\n",
      "movie|NOUN\n",
      "\t-> Sense2Vec: [('whole_movie|NOUN', np.float32(0.9032)), ('good_movie|NOUN', np.float32(0.9019)), ('terrible_movie|NOUN', np.float32(0.8995))]\n",
      "\t-> Word2Vec: [('film', 0.8676770329475403), ('movies', 0.8013108372688293), ('films', 0.7363011837005615)]\n",
      "\n",
      "------------------------\n",
      "He gave me a watch with a light.\n",
      "------------------------\n",
      "gave|VERB\n",
      "\t-> Sense2Vec: [('Gave|VERB', np.float32(0.8435)), ('giving|VERB', np.float32(0.8008)), ('had|VERB', np.float32(0.8002))]\n",
      "\t-> Word2Vec: [('give', 0.7412387728691101), ('giving', 0.7405864000320435), ('gives', 0.6644811630249023)]\n",
      "watch|NOUN\n",
      "\t-> Sense2Vec: [('wristwatch|NOUN', np.float32(0.7619)), ('watches|NOUN', np.float32(0.7581)), ('just_the_watch|NOUN', np.float32(0.7502))]\n",
      "\t-> Word2Vec: [('watching', 0.7835854291915894), ('watched', 0.6677262783050537), ('Watching', 0.6385796666145325)]\n",
      "light|NOUN\n",
      "\t-> Sense2Vec: [('_Light|NOUN', np.float32(0.859)), ('white_light|NOUN', np.float32(0.8365)), ('_light|NOUN', np.float32(0.8128))]\n",
      "\t-> Word2Vec: [('lights', 0.550593912601471), ('yellowish_glow', 0.5484952926635742), ('illumination', 0.5342711806297302)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"You must watch the light movie.\",\n",
    "    \"He gave me a watch with a light.\"\n",
    "]\n",
    "\n",
    "for sent in sentences:\n",
    "    print('------------------------')\n",
    "    print(sent)\n",
    "    print('------------------------')\n",
    "    for token in nlp(sent):\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            word = f'{token.text}|{token.pos_}'\n",
    "            print(word)\n",
    "            print('\\t-> Sense2Vec:', s2v.most_similar(word, n=3))\n",
    "            print('\\t-> Word2Vec:', w2v.most_similar(token.text, topn=3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "I went to the bank to deposit my money.\n",
      "------------------------\n",
      "went|VERB\n",
      "\t-> Sense2Vec: [('went|ADJ', np.float32(0.8834)), ('came|VERB', np.float32(0.8803)), (\"wen't|VERB\", np.float32(0.8792))]\n",
      "\t-> Word2Vec: [('came', 0.7141857743263245), ('ran', 0.671501874923706), ('gone', 0.6404926776885986)]\n",
      "bank|NOUN\n",
      "\t-> Sense2Vec: [('local_bank|NOUN', np.float32(0.8859)), ('bank_account|NOUN', np.float32(0.8653)), ('same_bank|NOUN', np.float32(0.8536))]\n",
      "\t-> Word2Vec: [('banks', 0.7440759539604187), ('banking', 0.690161406993866), ('Bank', 0.6698698401451111)]\n",
      "deposit|VERB\n",
      "\t-> Sense2Vec: [('depositing|VERB', np.float32(0.8951)), ('Deposit|VERB', np.float32(0.8719)), ('deposite|VERB', np.float32(0.7975))]\n",
      "\t-> Word2Vec: [('deposits', 0.8111314177513123), ('Deposit', 0.7686398029327393), ('Deposits', 0.6425759196281433)]\n",
      "money|NOUN\n",
      "\t-> Sense2Vec: [('_money|NOUN', np.float32(0.9145)), ('even_more_money|NOUN', np.float32(0.892)), ('own_money|NOUN', np.float32(0.8899))]\n",
      "\t-> Word2Vec: [('monies', 0.7165061831474304), ('funds', 0.7055202722549438), ('moneys', 0.6289054751396179)]\n",
      "\n",
      "------------------------\n",
      "The land along the river bank has vegetation.\n",
      "------------------------\n",
      "land|NOUN\n",
      "\t-> Sense2Vec: [('own_land|NOUN', np.float32(0.8291)), ('other_land|NOUN', np.float32(0.8124)), ('lands|NOUN', np.float32(0.7798))]\n",
      "\t-> Word2Vec: [('lands', 0.7458434104919434), ('farmland', 0.683372437953949), ('acres', 0.6153904795646667)]\n",
      "river|NOUN\n",
      "\t-> Sense2Vec: [('lake|NOUN', np.float32(0.877)), ('creek|NOUN', np.float32(0.8562)), ('shoreline|NOUN', np.float32(0.8504))]\n",
      "\t-> Word2Vec: [('creek', 0.7994443774223328), ('lake', 0.7919586896896362), ('rivers', 0.7777560949325562)]\n",
      "bank|NOUN\n",
      "\t-> Sense2Vec: [('local_bank|NOUN', np.float32(0.8859)), ('bank_account|NOUN', np.float32(0.8653)), ('same_bank|NOUN', np.float32(0.8536))]\n",
      "\t-> Word2Vec: [('banks', 0.7440759539604187), ('banking', 0.690161406993866), ('Bank', 0.6698698401451111)]\n",
      "vegetation|NOUN\n",
      "\t-> Sense2Vec: [('plant_life|NOUN', np.float32(0.9017)), ('grasses|NOUN', np.float32(0.8739)), ('foliage|NOUN', np.float32(0.8572))]\n",
      "\t-> Word2Vec: [('grasses', 0.7498385906219482), ('woody_vegetation', 0.7239639759063721), ('Vegetation', 0.7062167525291443)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"I went to the bank to deposit my money.\",\n",
    "    \"The land along the river bank has vegetation.\"\n",
    "]\n",
    "\n",
    "for sent in sentences:\n",
    "    print('------------------------')\n",
    "    print(sent)\n",
    "    print('------------------------')\n",
    "    for token in nlp(sent):\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            word = f'{token.text}|{token.pos_}'\n",
    "            print(word)\n",
    "            print('\\t-> Sense2Vec:', s2v.most_similar(word, n=3))\n",
    "            print('\\t-> Word2Vec:', w2v.most_similar(token.text, topn=3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watch_noun vs watch_verb [[0.47977123]]\n",
      "\n",
      "watch_noun vs clock_noun [[0.5055921]]\n",
      "watch_noun vs view_verb [[0.38052857]]\n",
      "\n",
      "watch_verb vs clock_noun [[0.32959634]]\n",
      "watch_verb vs view_verb [[0.43523753]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "watch_noun = s2v[\"watch|NOUN\"]\n",
    "watch_verb = s2v[\"watch|VERB\"]\n",
    "\n",
    "clock_noun = s2v[\"clock|NOUN\"]\n",
    "view_verb = s2v[\"view|VERB\"]\n",
    "\n",
    "print('watch_noun vs watch_verb', cosine_similarity([watch_noun], [watch_verb]))\n",
    "print()\n",
    "print('watch_noun vs clock_noun', cosine_similarity([watch_noun], [clock_noun]))\n",
    "print('watch_noun vs view_verb', cosine_similarity([watch_noun], [view_verb]))\n",
    "print()\n",
    "print('watch_verb vs clock_noun', cosine_similarity([watch_verb], [clock_noun]))\n",
    "print('watch_verb vs view_verb', cosine_similarity([watch_verb], [view_verb]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2: Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.1 y 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from datasets import load_dataset\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"laion/Wikipedia-Abstract\", \"English\")\n",
    "subset = dataset[\"train\"].select(range(10000))\n",
    "\n",
    "texts, titles = subset[\"Abstract\"], subset[\"Title\"]\n",
    "\n",
    "documents = []\n",
    "for text, title in zip(texts, titles):\n",
    "    documents.append(TaggedDocument(words=word_tokenize(text), tags=[title]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size=50, epochs=40, seed=42)\n",
    "model.build_vocab(documents)\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Federico García Lorca\n",
      "-------------------------------------------------\n",
      "[('Mohammad-Hossein Shahriar', 0.7323083281517029),\n",
      " ('Jacques de Mahieu', 0.7190210223197937),\n",
      " ('Esoteric Neo-Nazism', 0.6896402835845947),\n",
      " ('Victor Leemans', 0.6675578355789185),\n",
      " ('El Cid', 0.6634325385093689),\n",
      " (\"Ken'ichi Enomoto\", 0.6590982675552368),\n",
      " ('Benedetto Gennari', 0.6524226665496826),\n",
      " ('Jan Philip van Thielen', 0.6470993757247925),\n",
      " ('Albrecht Adam', 0.6456478238105774),\n",
      " ('Giuseppe Mazzuoli (c. 1536 – 1589)', 0.6403161287307739)]\n",
      "\n",
      "-------------------------------------------------\n",
      "Flag of Europe\n",
      "-------------------------------------------------\n",
      "[('Flag of Bohol', 0.7047033309936523),\n",
      " ('Flag of the Federal Territories', 0.6650753617286682),\n",
      " ('Accession of Albania to the European Union', 0.6437662839889526),\n",
      " ('Euroscepticism', 0.628987193107605),\n",
      " ('Serbia Davis Cup team', 0.5973030924797058),\n",
      " ('Treaty of Prague (1973)', 0.5958016514778137),\n",
      " ('1925 Australian federal election', 0.5859702825546265),\n",
      " ('Nothing about us without us', 0.5817858576774597),\n",
      " ('Flag of Romania', 0.5785704851150513),\n",
      " ('Nouvelle Résistance', 0.5773531794548035)]\n",
      "\n",
      "-------------------------------------------------\n",
      "Super Mario 64\n",
      "-------------------------------------------------\n",
      "[('Super Mario Land 2: 6 Golden Coins', 0.6632051467895508),\n",
      " ('Robert E. Lee: Civil War General', 0.648503303527832),\n",
      " (\"Everyone's Child\", 0.6407329440116882),\n",
      " ('Transformers: The Game', 0.629341185092926),\n",
      " ('Chess Assistant', 0.6242291331291199),\n",
      " ('Iron Dragon (board game)', 0.62184077501297),\n",
      " ('Kingdom Hearts III', 0.6080685257911682),\n",
      " ('Wario', 0.6069406270980835),\n",
      " ('Creature from the Black Lagoon (pinball)', 0.6028780937194824),\n",
      " ('Music of the Final Fantasy series', 0.6017530560493469)]\n"
     ]
    }
   ],
   "source": [
    "wiki_1 = \"Federico García Lorca\"\n",
    "print('-------------------------------------------------')\n",
    "print(wiki_1)\n",
    "print('-------------------------------------------------')\n",
    "pprint(model.dv.most_similar(positive=[wiki_1], topn=10))\n",
    "\n",
    "print()\n",
    "\n",
    "wiki_1 = \"Flag of Europe\"\n",
    "print('-------------------------------------------------')\n",
    "print(wiki_1)\n",
    "print('-------------------------------------------------')\n",
    "pprint(model.dv.most_similar(positive=[wiki_1], topn=10))\n",
    "\n",
    "print()\n",
    "\n",
    "wiki_1 = \"Super Mario 64\"\n",
    "print('-------------------------------------------------')\n",
    "print(wiki_1)\n",
    "print('-------------------------------------------------')\n",
    "pprint(model.dv.most_similar(positive=[wiki_1], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,) [-0.4026926   0.581282    0.40740594  0.0430668  -1.0033398  -1.4692134\n",
      " -0.43142968  1.0989631   0.42855978 -0.8597266  -0.29421526  0.8062698\n",
      "  0.17859367  2.3086727  -0.964565   -0.36739975 -1.1685674   1.0733256\n",
      "  0.6134671   1.4051841   1.7117182   0.18356495  0.32591483 -0.7381212\n",
      "  0.53039116  0.12287422 -0.07131319 -0.68314594  0.160554    0.61600083\n",
      " -0.23626818  0.65588164 -0.66059977  2.33449     0.5977025  -0.07788242\n",
      "  0.10233663 -0.6689951   0.6470793  -0.52845174  1.3158622   0.07683377\n",
      "  2.1582968  -0.3269521   0.10931224 -1.3920735   0.61149037  0.13574648\n",
      "  1.1888418  -1.7444515 ]\n",
      "\n",
      "[('Phytosulfokine', 0.7384074330329895),\n",
      " ('Sulfolobaceae', 0.6606782674789429),\n",
      " ('Grebe', 0.6436593532562256),\n",
      " ('Iridoid', 0.6341902017593384),\n",
      " ('Callus (cell biology)', 0.6155226230621338),\n",
      " ('Lyngbya', 0.6137983202934265),\n",
      " ('Lightweight programming language', 0.5977791547775269),\n",
      " ('Outline of zoology', 0.5888161063194275),\n",
      " ('Loxoscelism', 0.585954487323761),\n",
      " ('Methanospirillaceae', 0.5821808576583862)]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Mycology is the branch of biology concerned with the study of fungi, including their taxonomy, genetics, biochemical properties, and use by humans.[1] Fungi can be a source of tinder, food, traditional medicine, as well as entheogens, poison, and infection. Yeasts are among the most heavily utilized members of the Kingdom Fungi, particularly in food manufacturing.[2]\n",
    "\n",
    "Mycology branches into the field of phytopathology, the study of plant diseases. The two disciplines are closely related, because the vast majority of plant pathogens are fungi. A biologist specializing in mycology is called a mycologist\n",
    "\"\"\"\n",
    "\n",
    "vector = model.infer_vector(word_tokenize(text))\n",
    "\n",
    "print(vector.shape, vector)\n",
    "print()\n",
    "pprint(model.dv.most_similar([vector], topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, show\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "selection_subset = subset.select(range(500))\n",
    "texts, titles = list(selection_subset[\"Abstract\"]), list(selection_subset[\"Title\"])\n",
    "\n",
    "doc_vectors = [\n",
    "    model.infer_vector(word_tokenize(text))\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(doc_vectors)\n",
    "\n",
    "source = ColumnDataSource({'x': vectors_2d[:, 0], 'y': vectors_2d[:, 1], 'title': titles})\n",
    "\n",
    "p = figure(tools=\"pan,wheel_zoom,reset\", width=1700, height=1000)\n",
    "p.scatter(x='x', y='y', size=10, source=source, fill_alpha=0.6)\n",
    "p.text(x='x', y='y', text='title', source=source, x_offset=5, y_offset=5)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "03-representacion-texto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
